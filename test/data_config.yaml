ASR:
  azure_secrets_path: ''
  permit_ASR: false
SR: 16000
audio_feature_type: facebook/wav2vec2-base
calculate_kde: true
iemocap_description: The Interactive Emotional Dyadic Motion Capture (IEMOCAP) database
  is an acted, multimodal and multispeaker database, recently collected at SAIL lab
  at USC. It contains approximately 12 hours of audiovisual data, including video,
  speech, motion capture of face, text transcriptions. It consists of dyadic sessions
  where actors perform improvisations or scripted scenarios, specifically selected
  to elicit emotional expressions. IEMOCAP database is annotated by multiple annotators
  into categorical labels, such as anger, happiness, sadness, neutrality, as well
  as dimensional labels such as valence, activation and dominance. The detailed motion
  capture information, the interactive setting to elicit authentic emotions, and the
  size of the database make this corpus a valuable addition to the existing databases
  in the community for the study and modeling of multimodal and expressive human communication.
iemocap_directory: /z/public/data/IEMOCAP_full_release
improv_description: The MSP-Improv is an acted audiovisual emotional database that
  explores emotional behaviors during spontaneous dyadic improvisations. The scenarios
  are carefully designed to elicit realistic emotions. Currently, the corpus comprises
  data from six dyad sessions (12 actors). The participants are UTD students from
  the School of Arts and Humanities, who have taken classes in Theatre and Drama and
  have acting experience.
improv_directory: /z/public/data/MSP_IMPROV
max_len: -1
mfb_settings:
  clamp_val: 3.0
  clamp_values: true
  fmax: None
  fmin: 0
  hop_length: 160
  n_fft: 2048
  n_mels: 40
min_len: -1
muse_description: 'We collect a dataset that we refer to as Multimodal Stressed Emotion
  (MuSE) to facilitate the learning of the interplay between stress and emotion. There
  were two sections in each recording: monologues and watching emotionally evocative
  videos. We measure the stress level at the beginning and end of each recording.
  The monologue questions and videos were specifically chosen to cover all categories
  of emotions. At the start of each recording, we also recorded a short one-minute
  clip without any additional stimuli to register the baseline state of the subject.'
muse_directory: /z/public/data/MuSE
num_labels: 4
podcast_description: We are building the largest naturalistic speech emotional dataset
  in the community. The MSP-Podcast corpus contains speech segments from podcast recordings
  which are perceptually annotated using crowdsourcing. The collection of this corpus
  is an ongoing process. Version 1.11 of the corpus has 151,654 speaking turns (237
  hours and 56 mins). The proposed partition attempts to create speaker-independent
  datasets for Train, Development, Test1, Test2, and Test3 sets.
podcast_directory: /z/public/data/MSP_Podcast-1.11
segmented_iemocap_directory: /z/tavernor/mmfusion_storage/segmented_wavs
segmented_noisy_iemocap_directory: /z/tavernor/mmfusion_storage/iemocap_noisy_test/wavfile_test_dataset_ALL_segmented
text_feature_type: google/bert_uncased_L-4_H-512_A-8
use_whisper_for_muse: false
